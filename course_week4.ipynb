{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hse_course_week4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7qMgS4tPuxFqdPfKQ48fs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankovakris/hse_course_basic/blob/main/course_week4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4Ssyi64Lfrc"
      },
      "outputs": [],
      "source": [
        "#task0\n",
        "from sklearn.base import BaseEstimator\n",
        "import random\n",
        "\n",
        "class LinearRegressionSGD(BaseEstimator):\n",
        "    def __init__(self, epsilon=1e-6, max_steps=10000, w0=None, alpha=1e-8):\n",
        "        \"\"\"\n",
        "        epsilon: разница для нормы изменения весов \n",
        "        max_steps: максимальное количество шагов в градиентном спуске\n",
        "        w0: np.array (d,) - начальные веса\n",
        "        alpha: шаг обучения\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "        self.max_steps = max_steps\n",
        "        self.w0 = w0\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "        self.w_history = []\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        y: np.array (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        l, d = X.shape()\n",
        "        \n",
        "        if self.w0 is None:\n",
        "            self.w0 = np.zeros(d)\n",
        "        \n",
        "        self.w = self.w0\n",
        "        \n",
        "        for step in range(self.max_steps):\n",
        "            newx = []\n",
        "            newy = []\n",
        "            for i in range(10):\n",
        "                num = random.randint(0, len(y))\n",
        "                newx.append(X[num])\n",
        "                newy.append(y[num])\n",
        "            newx = np.array(newx)\n",
        "            newy = np.array(newy)\n",
        "            \n",
        "            self.w_history.append(self.w)\n",
        "            \n",
        "            w_new = self.w - self.alpha * self.calc_gradient(self, newx, newy)\n",
        "            \n",
        "            if (np.linalg.norm(w_new - self.w) < self.epsilon):\n",
        "                break\n",
        "                \n",
        "            self.w = w_new\n",
        "        return self\n",
        "        \n",
        "        ## На каждом шаге градиентного спуска веса можно добавлять в w_history (для последующей отрисовки)\n",
        "        ## Для выполнения шага выберите 10 случайных(равномерно) сэмплов\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        ---\n",
        "        output: np.array (l)\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.w is None:\n",
        "            raise Exception(\"Not trained yet\")\n",
        "        \n",
        "        l, d = X.shape\n",
        "        \n",
        "        y_pred = []\n",
        "        for i in range(l):\n",
        "            y_pred.append(np.dot(X[i], self.w))\n",
        "        \n",
        "        return np.array(y_pred)\n",
        "    \n",
        "    def calc_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array (l, d)\n",
        "        y: np.array (l)\n",
        "        ---\n",
        "        output: np.array (d)\n",
        "        \"\"\"\n",
        "        l, d = X.shape\n",
        "        gradient = []\n",
        "        \n",
        "        for j in range(d):\n",
        "          dQ = 0\n",
        "          for i in range(l):\n",
        "            dQ += (2/l) * X[i][j] * (np.dot(X[i], self.w) - y[i])\n",
        "          gradient.append(dQ)\n",
        "\n",
        "        return np.array(gradient)\n",
        "        return gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#task1\n",
        "def MAPE(y_true, y_pred):\n",
        "    \"\"\"\n",
        "        y_true: np.array (l)\n",
        "        y_pred: np.array (l)\n",
        "        ---\n",
        "        output: float [0, +inf)\n",
        "    \"\"\"\n",
        "    \n",
        "    mape = np.mean(np.abs((y_true - y_pred)/y_true))*100\n",
        "    return round(mape, 3)\n",
        "  \n",
        "y_0 = []\n",
        "for i in range(len(y_test)):\n",
        "    y_0.append(np.mean(y_test))\n",
        "y_0 = np.array(y_0)\n",
        "\n",
        "print(round(MAPE(y_test, y_0), 3))\n",
        "res_mape = round(MAPE(y_test, y_0), 3)\n",
        "\n",
        "\n",
        "# your code here\n"
      ],
      "metadata": {
        "id": "GGJs3h90LmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#task2???????????\n",
        "sgd = LinearRegressionSGD()\n",
        "X_train = np.array(X_train)\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "#y_pred_sgd = sdg.predict(X_test)\n",
        "\n",
        "#res_mape_SGD = MAPE(y_test, y_pred_sgd)\n",
        "\n",
        "\n",
        "# your code here\n"
      ],
      "metadata": {
        "id": "AzRrb2s4LvsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}